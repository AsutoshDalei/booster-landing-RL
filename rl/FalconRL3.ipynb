{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Ev8xYndcpuKJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LR = 0.00005\n",
    "entropy_coef = 0.01\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "K_EPOCHS = 10\n",
    "HIDDEN_DIM = 128\n",
    "ACTION_STD = 0.5\n",
    "\n",
    "# --- Physics Constants (Ported from JS) ---\n",
    "GRAVITY = 100.0\n",
    "THRUST_POWER = 300.0\n",
    "MASS = 1.0\n",
    "MOMENT_OF_INERTIA = 1000.0\n",
    "DRAG_COEFFICIENT = 0.05\n",
    "ANGULAR_DRAG = 2.0\n",
    "RCS_THRUST = 100.0  # Force applied by RCS (updated for force-based application)\n",
    "MIN_THROTTLE = 0.4  # Minimum throttle when engine is on (realistic rocket behavior)\n",
    "FRICTION = 0.5\n",
    "RESTITUTION = 0.0\n",
    "STOP_THRESHOLD = 0.5\n",
    "\n",
    "# --- Sim Constants ---\n",
    "DT = 1 / 60.0\n",
    "CANVAS_WIDTH = 800\n",
    "CANVAS_HEIGHT = 600\n",
    "PAD_X = 400\n",
    "PAD_Y = CANVAS_HEIGHT - 50\n",
    "PAD_WIDTH = 120\n",
    "ROCKET_WIDTH = 24\n",
    "ROCKET_HEIGHT = 100  # Updated to match JS physics.js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MbdwqIZyql0L"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, action_dim)\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, HIDDEN_DIM),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(HIDDEN_DIM, 1)\n",
    "        )\n",
    "        self.log_std = nn.Parameter(torch.zeros(action_dim))\n",
    "\n",
    "    def act(self, state):\n",
    "        mean = self.actor(state)\n",
    "        std = self.log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        raw_action = dist.sample()\n",
    "        action = torch.tanh(raw_action)\n",
    "        log_prob = dist.log_prob(raw_action).sum(-1)\n",
    "        return action, raw_action, log_prob, self.critic(state).squeeze(-1)\n",
    "\n",
    "    def evaluate(self, states, raw_actions):\n",
    "        mean = self.actor(states)\n",
    "        std = self.log_std.exp()\n",
    "        dist = Normal(mean, std)\n",
    "        log_probs = dist.log_prob(raw_actions).sum(-1)\n",
    "        return log_probs, self.critic(states).squeeze(-1), dist.entropy().sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KRkrr14SpuMF"
   },
   "outputs": [],
   "source": [
    "class FalconEnv:\n",
    "    def __init__(self):\n",
    "        self.state_dim = 6\n",
    "        self.action_dim = 3\n",
    "        self.prev_shaping = None\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Result: [x, y, vx, vy, angle, angularVelocity, throttle, gimbal, fuel, groundContact, landingResult, done, stepCount]\n",
    "        # Randomize \"Anywhere\" - matches JS state.js reset logic\n",
    "        # Random Start Conditions: Center +/- 150px (300px total range)\n",
    "        # Random initial Angle: 3 to 15 degrees (wider range)\n",
    "        angle_mag = np.random.uniform(3.0, 15.0) * (np.pi / 180.0)  # 3-15 degrees in radians\n",
    "        random_angle = angle_mag * (1 if np.random.random() < 0.5 else -1)\n",
    "\n",
    "        self.state = {\n",
    "            'x': PAD_X + (np.random.random() - 0.5) * 300.0,  # Center +/- 150 (matches JS)\n",
    "            'y': CANVAS_HEIGHT * 0.1,  # Start high (matches JS)\n",
    "            'vx': 0.0,  # No initial horizontal velocity (matches JS)\n",
    "            'vy': 50.0,  # Initial drop speed (matches JS)\n",
    "            'angle': random_angle,  # 3-15 degrees either direction (matches JS)\n",
    "            'angularVelocity': 0.0,  # No initial rotation (matches JS)\n",
    "            'throttle': 0.0,\n",
    "            'engineGimbal': 0.0,\n",
    "            'fuel': 100.0,\n",
    "            'groundContact': False,\n",
    "            'landingResult': None,\n",
    "            'done': False,\n",
    "            'stepCount': 0\n",
    "        }\n",
    "        self.prev_shaping = None\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Action: [throttle (0-1), gimbal (-1 to 1), rcs (-1 to 1)]\n",
    "        throttle, gimbal, rcs = action\n",
    "\n",
    "        # Clamp Inputs\n",
    "        throttle = np.clip(throttle, 0.0, 1.0)\n",
    "        gimbal = np.clip(gimbal, -1.0, 1.0)\n",
    "        rcs = np.clip(rcs, -1.0, 1.0)\n",
    "\n",
    "        s = self.state\n",
    "        s['throttle'] = throttle\n",
    "        s['engineGimbal'] = gimbal * 0.6  # Max 0.6 rad (updated to match JS autopilot.js)\n",
    "\n",
    "        # --- Physics Update ---\n",
    "        fx = 0.0\n",
    "        fy = GRAVITY * MASS\n",
    "        torque = 0.0\n",
    "\n",
    "        # Thrust with minimum throttle (matches JS physics.js)\n",
    "        if s['throttle'] > 0:\n",
    "            # Calculate power with minimum throttle (realistic rocket behavior)\n",
    "            # When throttle > 0, engine power is MIN_THROTTLE + throttle * (1 - MIN_THROTTLE)\n",
    "            power = MIN_THROTTLE + s['throttle'] * (1 - MIN_THROTTLE)\n",
    "            thrust_mag = power * THRUST_POWER * MASS\n",
    "            thrust_angle = s['angle'] + s['engineGimbal']\n",
    "\n",
    "            # Thrust vector components\n",
    "            fx += math.sin(thrust_angle) * thrust_mag\n",
    "            fy -= math.cos(thrust_angle) * thrust_mag\n",
    "\n",
    "            # Torque from gimbal (increased by 1.5x for better responsiveness, matches JS)\n",
    "            lever_arm = ROCKET_HEIGHT / 2.0\n",
    "            torque += -lever_arm * thrust_mag * math.sin(s['engineGimbal']) * 1.5\n",
    "\n",
    "        # RCS Force Application (at thruster position, matches JS physics.js)\n",
    "        # RCS thrusters are at the top of the rocket and apply force perpendicular to rocket axis\n",
    "        if abs(rcs) > 0.3:  # Deadband threshold\n",
    "            # Calculate thruster position (top of rocket, 86% of height from center)\n",
    "            THRUSTER_HEIGHT = ROCKET_HEIGHT * 0.86\n",
    "\n",
    "            # Force direction: perpendicular to rocket axis\n",
    "            # rcs < -0.3 (left) pushes rocket right (positive X)\n",
    "            # rcs > 0.3 (right) pushes rocket left (negative X)\n",
    "            force_dir = 1.0 if rcs < -0.3 else -1.0\n",
    "\n",
    "            # Force components perpendicular to rocket axis\n",
    "            force_x = force_dir * math.cos(s['angle']) * RCS_THRUST\n",
    "            force_y = force_dir * math.sin(s['angle']) * RCS_THRUST\n",
    "\n",
    "            # Apply force at center of mass (simplified)\n",
    "            fx += force_x\n",
    "            fy += force_y\n",
    "\n",
    "            # Force at thruster position also creates torque\n",
    "            lever_arm = THRUSTER_HEIGHT\n",
    "            force_perpendicular = force_dir * RCS_THRUST\n",
    "            # Torque sign: rcsLeft (forceDir=1) creates CCW rotation (negative torque)\n",
    "            torque += -lever_arm * force_perpendicular\n",
    "\n",
    "        # Drag\n",
    "        fx -= s['vx'] * DRAG_COEFFICIENT\n",
    "        fy -= s['vy'] * DRAG_COEFFICIENT\n",
    "        torque -= s['angularVelocity'] * ANGULAR_DRAG\n",
    "\n",
    "        # Integration\n",
    "        s['vx'] += (fx / MASS) * DT\n",
    "        s['vy'] += (fy / MASS) * DT\n",
    "        s['angularVelocity'] += (torque / MOMENT_OF_INERTIA) * DT\n",
    "\n",
    "        s['x'] += s['vx'] * DT\n",
    "        s['y'] += s['vy'] * DT\n",
    "        s['angle'] += s['angularVelocity'] * DT\n",
    "\n",
    "        # Collision\n",
    "        ground_y = PAD_Y - 10 # Approx pad top\n",
    "        rocket_bottom = s['y'] + ROCKET_HEIGHT / 2.0\n",
    "\n",
    "        if rocket_bottom >= ground_y:\n",
    "            s['groundContact'] = True\n",
    "            s['y'] = ground_y - ROCKET_HEIGHT / 2.0\n",
    "\n",
    "            if s['vy'] > 0:\n",
    "                s['vx'] *= FRICTION\n",
    "                s['angularVelocity'] *= 0.8\n",
    "                if s['vy'] < STOP_THRESHOLD:\n",
    "                    s['vy'] = 0.0\n",
    "                else:\n",
    "                    s['vy'] = -s['vy'] * RESTITUTION\n",
    "\n",
    "        # --- Outcome Detection ---\n",
    "        # Out of bounds\n",
    "        if s['x'] < 0 or s['x'] > CANVAS_WIDTH or s['y'] > CANVAS_HEIGHT + 100 or s['y'] < -500:\n",
    "            s['landingResult'] = 'FAILURE'\n",
    "            s['done'] = True\n",
    "\n",
    "        # Replace the \"Landing Logic\" block in step()\n",
    "        # -------- Replace the Landing Logic in step() --------\n",
    "        if s['groundContact'] and not s['done']:\n",
    "            # Relaxed constraints to let the model learn \"almost\" landings\n",
    "            angle_deg = abs(s['angle'] * 180.0 / math.pi)\n",
    "            on_pad = abs(s['x'] - PAD_X) < (PAD_WIDTH * 0.8) # Generous pad width\n",
    "\n",
    "            # Widen the success window:\n",
    "            # - Speed < 5.0 (was 1.0)\n",
    "            # - Angle < 15.0 (was 5.0)\n",
    "            is_stable = angle_deg < 15.0\n",
    "            is_slow = abs(s['vy']) < 5.0 and abs(s['vx']) < 5.0\n",
    "\n",
    "            if on_pad and is_stable and is_slow:\n",
    "                s['landingResult'] = 'SUCCESS'\n",
    "            else:\n",
    "                s['landingResult'] = 'FAILURE'\n",
    "\n",
    "            # End episode immediately on contact\n",
    "            s['done'] = True\n",
    "\n",
    "        # Timeout\n",
    "        s['stepCount'] += 1\n",
    "        if s['stepCount'] > 1000:\n",
    "            s['landingResult'] = 'FAILURE'\n",
    "            s['done'] = True\n",
    "\n",
    "        reward = self._compute_reward()\n",
    "\n",
    "        return self._get_obs(), reward, s['done'], {'result': s['landingResult']}\n",
    "\n",
    "    def _get_obs(self):\n",
    "        s = self.state\n",
    "        # Wrap angle to [-pi, pi]\n",
    "        wrapped_angle = (s['angle'] + math.pi) % (2 * math.pi) - math.pi\n",
    "        return np.array([\n",
    "            (s['x'] - PAD_X) / 200.0,\n",
    "            (PAD_Y - s['y']) / 400.0,\n",
    "            s['vx'] / 50.0,\n",
    "            s['vy'] / 50.0,\n",
    "            wrapped_angle / math.pi, # Normalized -1 to 1\n",
    "            s['angularVelocity'] / 10.0\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "    def _compute_reward(self):\n",
    "        s = self.state\n",
    "        TARGET_X = PAD_X\n",
    "        TARGET_Y = PAD_Y - ROCKET_HEIGHT / 2.0\n",
    "\n",
    "        dist_x = abs(s['x'] - TARGET_X) / CANVAS_WIDTH\n",
    "        dist_y = abs(s['y'] - TARGET_Y) / CANVAS_HEIGHT\n",
    "\n",
    "        vel_penalty = math.sqrt(s['vx']**2 + s['vy']**2) / 100.0\n",
    "        angle_penalty = abs(s['angle'])\n",
    "\n",
    "        shaping = -100 * math.sqrt(dist_x**2 + dist_y**2) \\\n",
    "                  - 100 * vel_penalty \\\n",
    "                  - 100 * angle_penalty\n",
    "\n",
    "        if self.prev_shaping is not None:\n",
    "            reward = shaping - self.prev_shaping\n",
    "        else:\n",
    "            reward = 0\n",
    "        self.prev_shaping = shaping\n",
    "\n",
    "        if s['landingResult'] == 'FAILURE': return reward - 100.0\n",
    "        if s['landingResult'] == 'SUCCESS': return reward + 500.0 # Increased\n",
    "\n",
    "        return reward - 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "wHf4WXxiqxoO"
   },
   "outputs": [],
   "source": [
    "def compute_gae(rewards, values, dones, next_value, gamma=0.99, lam=0.95):\n",
    "    advantages = []\n",
    "    gae = 0\n",
    "    values = values + [next_value]\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * values[t+1] * (1 - dones[t]) - values[t]\n",
    "        gae = delta + gamma * lam * (1 - dones[t]) * gae\n",
    "        advantages.insert(0, gae)\n",
    "\n",
    "    returns = [adv + val for adv, val in zip(advantages, values[:-1])]\n",
    "    return torch.tensor(advantages), torch.tensor(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qMMI3Zt9rj6o"
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    env = FalconEnv()\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    policy = ActorCritic(env.state_dim, env.action_dim).to(device)\n",
    "    optimizer = optim.Adam(policy.parameters(), lr=0.0001)\n",
    "\n",
    "    clip_eps = 0.2\n",
    "    entropy_coef = 0.05\n",
    "    best_success = 0\n",
    "\n",
    "    state = env.reset()\n",
    "    success_history = []\n",
    "\n",
    "    for episode in range(1, 1001):\n",
    "        states, raw_actions, logprobs, rewards, dones, values = [], [], [], [], [], []\n",
    "        ep_reward = 0\n",
    "        success_count = 0\n",
    "\n",
    "        for _ in range(2048):\n",
    "            st = torch.tensor(state, dtype=torch.float32).to(device)\n",
    "            with torch.no_grad():\n",
    "                action, raw_a, lp, val = policy.act(st)\n",
    "\n",
    "            a_env = action.cpu().numpy()\n",
    "            next_state, reward, done, info = env.step([(a_env[0]+1)/2, a_env[1], a_env[2]])\n",
    "\n",
    "            states.append(st)\n",
    "            raw_actions.append(raw_a)\n",
    "            logprobs.append(lp)\n",
    "            dones.append(done)\n",
    "            values.append(val.item())\n",
    "            rewards.append(reward)\n",
    "            ep_reward += reward\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                if info.get('result') == 'SUCCESS':\n",
    "                    success_count += 1\n",
    "                state = env.reset()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_val = policy.critic(torch.tensor(state, dtype=torch.float32).to(device)).item()\n",
    "        adv, ret = compute_gae(rewards, values, dones, next_val, GAMMA)\n",
    "        adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        b_s = torch.stack(states)\n",
    "        b_a = torch.stack(raw_actions)\n",
    "        b_lp = torch.stack(logprobs)\n",
    "        b_ret = ret.to(device)\n",
    "        b_adv = adv.to(device)\n",
    "\n",
    "        for _ in range(K_EPOCHS):\n",
    "            inds = np.arange(2048)\n",
    "            np.random.shuffle(inds)\n",
    "            for i in range(0, 2048, BATCH_SIZE):\n",
    "                sb = inds[i:i+BATCH_SIZE]\n",
    "                lp_n, val_n, ent = policy.evaluate(b_s[sb], b_a[sb])\n",
    "                ratio = torch.exp(lp_n - b_lp[sb])\n",
    "\n",
    "                surr1 = ratio * b_adv[sb]\n",
    "                surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * b_adv[sb]\n",
    "\n",
    "                loss = -torch.min(surr1, surr2).mean() + 0.5 * (b_ret[sb] - val_n).pow(2).mean() - entropy_coef * ent.mean()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(policy.parameters(), max_norm=0.5)\n",
    "                optimizer.step()\n",
    "\n",
    "        if success_count > best_success:\n",
    "            best_success = success_count\n",
    "            torch.save(policy.state_dict(), \"falcon_ppo_best.pth\")\n",
    "            print(f\"New Best Model Saved! Successes: {best_success}\")\n",
    "\n",
    "        success_history.append(success_count)\n",
    "\n",
    "        if episode % 5 == 0:\n",
    "            avg_success = (sum(success_history[-10:]) / 10)\n",
    "\n",
    "            if avg_success > 1.5:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    if param_group['lr'] > 0.00001:\n",
    "                        param_group['lr'] = 0.00001\n",
    "                        print(\"LR Reduced to 1e-5\")\n",
    "\n",
    "            dist_x = abs(next_state[0] * 200.0)\n",
    "            last_vx = next_state[2] * 50.0\n",
    "            last_vy = next_state[3] * 50.0\n",
    "            last_throttle = (a_env[0] + 1) / 2\n",
    "\n",
    "            print(f\"Ep: {episode} | Rew: {ep_reward:.1f} | SR: {avg_success:.1f} | Vy: {last_vy:.1f} | Thr: {last_throttle:.2f} | DistX: {dist_x:.1f}\")\n",
    "\n",
    "    torch.save(policy.state_dict(), \"falcon_ppo_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "w6g8ndOkpuO7"
   },
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "# train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 690
    },
    "id": "wL3DWV3QpuRS",
    "outputId": "b0dfc081-2388-4930-fd67-f0b68225351a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exporting model with input shape: torch.Size([1, 6])\n",
      "⚠️  CRITICAL: Model will output tanh(mean) in [-1, 1] range\n",
      "   This fixes the issue where raw means (like -9.08) cause throttle=0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vg/py587mbx6nq4z2vh3d9b_5780000gn/T/ipykernel_57893/1264010780.py:46: DeprecationWarning: You are using the legacy TorchScript-based ONNX export. Starting in PyTorch 2.9, the new torch.export-based ONNX exporter will be the default. To switch now, set dynamo=True in torch.onnx.export. This new exporter supports features like exporting LLMs with DynamicCache. We encourage you to try it and share feedback to help improve the experience. Learn more about the new export logic: https://pytorch.org/docs/stable/onnx_dynamo.html. For exporting control flow: https://pytorch.org/tutorials/beginner/onnx/export_control_flow_model_to_onnx_tutorial.html.\n",
      "  torch.onnx.export(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Model exported to: /Users/asutoshdalei/Desktop/Work/Game2/rl/falcon_ppo_best.onnx\n",
      "   Model outputs are now in [-1, 1] range (tanh already applied)\n",
      "   JavaScript will detect values are in range and use them directly\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# from train_falcon_rl import ActorCritic, FalconEnv\n",
    "\n",
    "def export_to_onnx():\n",
    "    # 1. Config - IMPORTANT: Use the best model\n",
    "    MODEL_PATH = \"/Users/asutoshdalei/Desktop/Work/Game2/rl/falcon_ppo_best.pth\"  # Use best model, not just falcon_ppo.pth\n",
    "    ONNX_PATH = \"/Users/asutoshdalei/Desktop/Work/Game2/rl/falcon_ppo_best.onnx\"  # Match the model name\n",
    "\n",
    "    # 2. Setup Environment & Model\n",
    "    env = FalconEnv()\n",
    "    state_dim = env.state_dim\n",
    "    action_dim = env.action_dim\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "    model = ActorCritic(state_dim, action_dim)\n",
    "    state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.eval()\n",
    "    model.actor.float()\n",
    "\n",
    "    # 3. CRITICAL FIX: Create a wrapper that includes tanh\n",
    "    # During training: action = tanh(sample(Normal(mean, std)))\n",
    "    # During inference: we want tanh(mean) to get deterministic action in [-1, 1]\n",
    "    # Without this, the model outputs raw means (like -9.08) which tanh to -1, causing throttle=0\n",
    "    class ActorWithTanh(nn.Module):\n",
    "        def __init__(self, actor):\n",
    "            super().__init__()\n",
    "            self.actor = actor\n",
    "        \n",
    "        def forward(self, x):\n",
    "            mean = self.actor(x)\n",
    "            return torch.tanh(mean)  # Apply tanh to get actions in [-1, 1]\n",
    "    \n",
    "    actor_with_tanh = ActorWithTanh(model.actor)\n",
    "    actor_with_tanh.eval()\n",
    "\n",
    "    # 4. Create Dummy Input\n",
    "    dummy_input = torch.randn(1, state_dim, dtype=torch.float32, device=device)\n",
    "\n",
    "    print(f\"Exporting model with input shape: {dummy_input.shape}\")\n",
    "    print(\"⚠️  CRITICAL: Model will output tanh(mean) in [-1, 1] range\")\n",
    "    print(\"   This fixes the issue where raw means (like -9.08) cause throttle=0\")\n",
    "\n",
    "    # 5. Export with tanh included\n",
    "    torch.onnx.export(\n",
    "        actor_with_tanh,          # Export wrapper with tanh (FIXED)\n",
    "        dummy_input,\n",
    "        ONNX_PATH,\n",
    "        export_params=True,\n",
    "        opset_version=11,\n",
    "        do_constant_folding=True,\n",
    "        input_names=['input.1'],   # Match JS input name\n",
    "        output_names=['output.1'], # Match JS output name\n",
    "        dynamic_axes={\n",
    "            'input.1': {0: 'batch_size'},\n",
    "            'output.1': {0: 'batch_size'}\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(f\"✅ Success! Model exported to: {ONNX_PATH}\")\n",
    "    print(\"   Model outputs are now in [-1, 1] range (tanh already applied)\")\n",
    "    print(\"   JavaScript will detect values are in range and use them directly\")\n",
    "\n",
    "export_to_onnx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rpRFmoZZpuTm",
    "outputId": "4c262dae-2972-4ec3-d6cb-1666c2ee363f"
   },
   "outputs": [],
   "source": [
    "!pip3 install onnx onnxscript onnxruntime --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "hcKIgC11puWN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/asutoshdalei/Desktop/Work/Game2/rl'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FfYxB7iGpuYi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1IFEzDapubK"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
